(grconv) xinda-wang@xinda-wang-Legion-Y9000P-IRX8:~/Desktop/robotic-grasping$ python train_network.py --network s1 --dataset cornell --dataset-path ../Mamba/archive --description training_cornell
root        : INFO     CUDA detected. Running with GPU acceleration.
root        : INFO     Loading Cornell Dataset...
root        : INFO     Dataset size is 885
root        : INFO     Training size: 796
root        : INFO     Validation size: 89
root        : INFO     Done
root        : INFO     Loading Network...
root        : INFO     Done
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
          Identity-1          [-1, 4, 224, 224]               0
            Conv2d-2         [-1, 16, 112, 112]             576
       BatchNorm2d-3         [-1, 16, 112, 112]              32
              ReLU-4         [-1, 16, 112, 112]               0
         ConvLayer-5         [-1, 16, 112, 112]               0
          Identity-6         [-1, 16, 112, 112]               0
            Conv2d-7         [-1, 16, 112, 112]           2,304
       BatchNorm2d-8         [-1, 16, 112, 112]              32
              ReLU-9         [-1, 16, 112, 112]               0
        ConvLayer-10         [-1, 16, 112, 112]               0
         Identity-11         [-1, 16, 112, 112]               0
           Conv2d-12         [-1, 16, 112, 112]           2,304
      BatchNorm2d-13         [-1, 16, 112, 112]              32
         Identity-14         [-1, 16, 112, 112]               0
        ConvLayer-15         [-1, 16, 112, 112]               0
         Identity-16         [-1, 16, 112, 112]               0
           Conv2d-17          [-1, 128, 56, 56]          18,432
      BatchNorm2d-18          [-1, 128, 56, 56]             256
             ReLU-19          [-1, 128, 56, 56]               0
        ConvLayer-20          [-1, 128, 56, 56]               0
         Identity-21          [-1, 128, 56, 56]               0
           Conv2d-22           [-1, 32, 56, 56]           4,096
      BatchNorm2d-23           [-1, 32, 56, 56]              64
         Identity-24           [-1, 32, 56, 56]               0
        ConvLayer-25           [-1, 32, 56, 56]               0
             Stem-26             [-1, 3136, 32]               0
           Conv2d-27           [-1, 32, 56, 56]             320
        LayerNorm-28             [-1, 3136, 32]              64
           Mamba2-29             [-1, 3136, 32]               0
         Identity-30             [-1, 3136, 32]               0
           Conv2d-31           [-1, 32, 56, 56]             320
        LayerNorm-32             [-1, 3136, 32]              64
           Linear-33            [-1, 3136, 128]           4,224
             GELU-34            [-1, 3136, 128]               0
          Dropout-35            [-1, 3136, 128]               0
           Linear-36             [-1, 3136, 32]           4,128
          Dropout-37             [-1, 3136, 32]               0
              Mlp-38             [-1, 3136, 32]               0
         Identity-39             [-1, 3136, 32]               0
     VMAMBA2Block-40             [-1, 3136, 32]               0
           Conv2d-41           [-1, 32, 56, 56]             320
        LayerNorm-42             [-1, 3136, 32]              64
           Mamba2-43             [-1, 3136, 32]               0
         DropPath-44             [-1, 3136, 32]               0
           Conv2d-45           [-1, 32, 56, 56]             320
        LayerNorm-46             [-1, 3136, 32]              64
           Linear-47            [-1, 3136, 128]           4,224
             GELU-48            [-1, 3136, 128]               0
          Dropout-49            [-1, 3136, 128]               0
           Linear-50             [-1, 3136, 32]           4,128
          Dropout-51             [-1, 3136, 32]               0
              Mlp-52             [-1, 3136, 32]               0
         DropPath-53             [-1, 3136, 32]               0
     VMAMBA2Block-54             [-1, 3136, 32]               0
         Identity-55           [-1, 32, 56, 56]               0
           Conv2d-56          [-1, 256, 56, 56]           8,448
         Identity-57          [-1, 256, 56, 56]               0
             ReLU-58          [-1, 256, 56, 56]               0
        ConvLayer-59          [-1, 256, 56, 56]               0
         Identity-60          [-1, 256, 56, 56]               0
           Conv2d-61          [-1, 256, 28, 28]           2,560
         Identity-62          [-1, 256, 28, 28]               0
             ReLU-63          [-1, 256, 28, 28]               0
        ConvLayer-64          [-1, 256, 28, 28]               0
         Identity-65          [-1, 256, 28, 28]               0
           Conv2d-66           [-1, 64, 28, 28]          16,448
      BatchNorm2d-67           [-1, 64, 28, 28]             128
         Identity-68           [-1, 64, 28, 28]               0
        ConvLayer-69           [-1, 64, 28, 28]               0
     PatchMerging-70              [-1, 784, 64]               0
       BasicLayer-71  [[-1, 3136, 32], [-1, 784, 64]]               0
        LayerNorm-72             [-1, 3136, 32]              64
           Conv2d-73           [-1, 64, 28, 28]             640
        LayerNorm-74              [-1, 784, 64]             128
           Mamba2-75              [-1, 784, 64]               0
         DropPath-76              [-1, 784, 64]               0
           Conv2d-77           [-1, 64, 28, 28]             640
        LayerNorm-78              [-1, 784, 64]             128
           Linear-79             [-1, 784, 256]          16,640
             GELU-80             [-1, 784, 256]               0
          Dropout-81             [-1, 784, 256]               0
           Linear-82              [-1, 784, 64]          16,448
          Dropout-83              [-1, 784, 64]               0
              Mlp-84              [-1, 784, 64]               0
         DropPath-85              [-1, 784, 64]               0
     VMAMBA2Block-86              [-1, 784, 64]               0
           Conv2d-87           [-1, 64, 28, 28]             640
        LayerNorm-88              [-1, 784, 64]             128
           Mamba2-89              [-1, 784, 64]               0
         DropPath-90              [-1, 784, 64]               0
           Conv2d-91           [-1, 64, 28, 28]             640
        LayerNorm-92              [-1, 784, 64]             128
           Linear-93             [-1, 784, 256]          16,640
             GELU-94             [-1, 784, 256]               0
          Dropout-95             [-1, 784, 256]               0
           Linear-96              [-1, 784, 64]          16,448
          Dropout-97              [-1, 784, 64]               0
              Mlp-98              [-1, 784, 64]               0
         DropPath-99              [-1, 784, 64]               0
    VMAMBA2Block-100              [-1, 784, 64]               0
          Conv2d-101           [-1, 64, 28, 28]             640
       LayerNorm-102              [-1, 784, 64]             128
          Mamba2-103              [-1, 784, 64]               0
        DropPath-104              [-1, 784, 64]               0
          Conv2d-105           [-1, 64, 28, 28]             640
       LayerNorm-106              [-1, 784, 64]             128
          Linear-107             [-1, 784, 256]          16,640
            GELU-108             [-1, 784, 256]               0
         Dropout-109             [-1, 784, 256]               0
          Linear-110              [-1, 784, 64]          16,448
         Dropout-111              [-1, 784, 64]               0
             Mlp-112              [-1, 784, 64]               0
        DropPath-113              [-1, 784, 64]               0
    VMAMBA2Block-114              [-1, 784, 64]               0
          Conv2d-115           [-1, 64, 28, 28]             640
       LayerNorm-116              [-1, 784, 64]             128
          Mamba2-117              [-1, 784, 64]               0
        DropPath-118              [-1, 784, 64]               0
          Conv2d-119           [-1, 64, 28, 28]             640
       LayerNorm-120              [-1, 784, 64]             128
          Linear-121             [-1, 784, 256]          16,640
            GELU-122             [-1, 784, 256]               0
         Dropout-123             [-1, 784, 256]               0
          Linear-124              [-1, 784, 64]          16,448
         Dropout-125              [-1, 784, 64]               0
             Mlp-126              [-1, 784, 64]               0
        DropPath-127              [-1, 784, 64]               0
    VMAMBA2Block-128              [-1, 784, 64]               0
        Identity-129           [-1, 64, 28, 28]               0
          Conv2d-130          [-1, 512, 28, 28]          33,280
        Identity-131          [-1, 512, 28, 28]               0
            ReLU-132          [-1, 512, 28, 28]               0
       ConvLayer-133          [-1, 512, 28, 28]               0
        Identity-134          [-1, 512, 28, 28]               0
          Conv2d-135          [-1, 512, 14, 14]           5,120
        Identity-136          [-1, 512, 14, 14]               0
            ReLU-137          [-1, 512, 14, 14]               0
       ConvLayer-138          [-1, 512, 14, 14]               0
        Identity-139          [-1, 512, 14, 14]               0
          Conv2d-140          [-1, 128, 14, 14]          65,664
     BatchNorm2d-141          [-1, 128, 14, 14]             256
        Identity-142          [-1, 128, 14, 14]               0
       ConvLayer-143          [-1, 128, 14, 14]               0
    PatchMerging-144             [-1, 196, 128]               0
      BasicLayer-145  [[-1, 784, 64], [-1, 196, 128]]               0
       LayerNorm-146              [-1, 784, 64]             128
          Conv2d-147          [-1, 128, 14, 14]           1,280
       LayerNorm-148             [-1, 196, 128]             256
          Mamba2-149             [-1, 196, 128]               0
        DropPath-150             [-1, 196, 128]               0
          Conv2d-151          [-1, 128, 14, 14]           1,280
       LayerNorm-152             [-1, 196, 128]             256
          Linear-153             [-1, 196, 512]          66,048
            GELU-154             [-1, 196, 512]               0
         Dropout-155             [-1, 196, 512]               0
          Linear-156             [-1, 196, 128]          65,664
         Dropout-157             [-1, 196, 128]               0
             Mlp-158             [-1, 196, 128]               0
        DropPath-159             [-1, 196, 128]               0
    VMAMBA2Block-160             [-1, 196, 128]               0
          Conv2d-161          [-1, 128, 14, 14]           1,280
       LayerNorm-162             [-1, 196, 128]             256
          Mamba2-163             [-1, 196, 128]               0
        DropPath-164             [-1, 196, 128]               0
          Conv2d-165          [-1, 128, 14, 14]           1,280
       LayerNorm-166             [-1, 196, 128]             256
          Linear-167             [-1, 196, 512]          66,048
            GELU-168             [-1, 196, 512]               0
         Dropout-169             [-1, 196, 512]               0
          Linear-170             [-1, 196, 128]          65,664
         Dropout-171             [-1, 196, 128]               0
             Mlp-172             [-1, 196, 128]               0
        DropPath-173             [-1, 196, 128]               0
    VMAMBA2Block-174             [-1, 196, 128]               0
          Conv2d-175          [-1, 128, 14, 14]           1,280
       LayerNorm-176             [-1, 196, 128]             256
          Mamba2-177             [-1, 196, 128]               0
        DropPath-178             [-1, 196, 128]               0
          Conv2d-179          [-1, 128, 14, 14]           1,280
       LayerNorm-180             [-1, 196, 128]             256
          Linear-181             [-1, 196, 512]          66,048
            GELU-182             [-1, 196, 512]               0
         Dropout-183             [-1, 196, 512]               0
          Linear-184             [-1, 196, 128]          65,664
         Dropout-185             [-1, 196, 128]               0
             Mlp-186             [-1, 196, 128]               0
        DropPath-187             [-1, 196, 128]               0
    VMAMBA2Block-188             [-1, 196, 128]               0
          Conv2d-189          [-1, 128, 14, 14]           1,280
       LayerNorm-190             [-1, 196, 128]             256
          Mamba2-191             [-1, 196, 128]               0
        DropPath-192             [-1, 196, 128]               0
          Conv2d-193          [-1, 128, 14, 14]           1,280
       LayerNorm-194             [-1, 196, 128]             256
          Linear-195             [-1, 196, 512]          66,048
            GELU-196             [-1, 196, 512]               0
         Dropout-197             [-1, 196, 512]               0
          Linear-198             [-1, 196, 128]          65,664
         Dropout-199             [-1, 196, 128]               0
             Mlp-200             [-1, 196, 128]               0
        DropPath-201             [-1, 196, 128]               0
    VMAMBA2Block-202             [-1, 196, 128]               0
          Conv2d-203          [-1, 128, 14, 14]           1,280
       LayerNorm-204             [-1, 196, 128]             256
          Mamba2-205             [-1, 196, 128]               0
        DropPath-206             [-1, 196, 128]               0
          Conv2d-207          [-1, 128, 14, 14]           1,280
       LayerNorm-208             [-1, 196, 128]             256
          Linear-209             [-1, 196, 512]          66,048
            GELU-210             [-1, 196, 512]               0
         Dropout-211             [-1, 196, 512]               0
          Linear-212             [-1, 196, 128]          65,664
         Dropout-213             [-1, 196, 128]               0
             Mlp-214             [-1, 196, 128]               0
        DropPath-215             [-1, 196, 128]               0
    VMAMBA2Block-216             [-1, 196, 128]               0
          Conv2d-217          [-1, 128, 14, 14]           1,280
       LayerNorm-218             [-1, 196, 128]             256
          Mamba2-219             [-1, 196, 128]               0
        DropPath-220             [-1, 196, 128]               0
          Conv2d-221          [-1, 128, 14, 14]           1,280
       LayerNorm-222             [-1, 196, 128]             256
          Linear-223             [-1, 196, 512]          66,048
            GELU-224             [-1, 196, 512]               0
         Dropout-225             [-1, 196, 512]               0
          Linear-226             [-1, 196, 128]          65,664
         Dropout-227             [-1, 196, 128]               0
             Mlp-228             [-1, 196, 128]               0
        DropPath-229             [-1, 196, 128]               0
    VMAMBA2Block-230             [-1, 196, 128]               0
          Conv2d-231          [-1, 128, 14, 14]           1,280
       LayerNorm-232             [-1, 196, 128]             256
          Mamba2-233             [-1, 196, 128]               0
        DropPath-234             [-1, 196, 128]               0
          Conv2d-235          [-1, 128, 14, 14]           1,280
       LayerNorm-236             [-1, 196, 128]             256
          Linear-237             [-1, 196, 512]          66,048
            GELU-238             [-1, 196, 512]               0
         Dropout-239             [-1, 196, 512]               0
          Linear-240             [-1, 196, 128]          65,664
         Dropout-241             [-1, 196, 128]               0
             Mlp-242             [-1, 196, 128]               0
        DropPath-243             [-1, 196, 128]               0
    VMAMBA2Block-244             [-1, 196, 128]               0
          Conv2d-245          [-1, 128, 14, 14]           1,280
       LayerNorm-246             [-1, 196, 128]             256
          Mamba2-247             [-1, 196, 128]               0
        DropPath-248             [-1, 196, 128]               0
          Conv2d-249          [-1, 128, 14, 14]           1,280
       LayerNorm-250             [-1, 196, 128]             256
          Linear-251             [-1, 196, 512]          66,048
            GELU-252             [-1, 196, 512]               0
         Dropout-253             [-1, 196, 512]               0
          Linear-254             [-1, 196, 128]          65,664
         Dropout-255             [-1, 196, 128]               0
             Mlp-256             [-1, 196, 128]               0
        DropPath-257             [-1, 196, 128]               0
    VMAMBA2Block-258             [-1, 196, 128]               0
          Conv2d-259          [-1, 128, 14, 14]           1,280
       LayerNorm-260             [-1, 196, 128]             256
          Mamba2-261             [-1, 196, 128]               0
        DropPath-262             [-1, 196, 128]               0
          Conv2d-263          [-1, 128, 14, 14]           1,280
       LayerNorm-264             [-1, 196, 128]             256
          Linear-265             [-1, 196, 512]          66,048
            GELU-266             [-1, 196, 512]               0
         Dropout-267             [-1, 196, 512]               0
          Linear-268             [-1, 196, 128]          65,664
         Dropout-269             [-1, 196, 128]               0
             Mlp-270             [-1, 196, 128]               0
        DropPath-271             [-1, 196, 128]               0
    VMAMBA2Block-272             [-1, 196, 128]               0
          Conv2d-273          [-1, 128, 14, 14]           1,280
       LayerNorm-274             [-1, 196, 128]             256
          Mamba2-275             [-1, 196, 128]               0
        DropPath-276             [-1, 196, 128]               0
          Conv2d-277          [-1, 128, 14, 14]           1,280
       LayerNorm-278             [-1, 196, 128]             256
          Linear-279             [-1, 196, 512]          66,048
            GELU-280             [-1, 196, 512]               0
         Dropout-281             [-1, 196, 512]               0
          Linear-282             [-1, 196, 128]          65,664
         Dropout-283             [-1, 196, 128]               0
             Mlp-284             [-1, 196, 128]               0
        DropPath-285             [-1, 196, 128]               0
    VMAMBA2Block-286             [-1, 196, 128]               0
          Conv2d-287          [-1, 128, 14, 14]           1,280
       LayerNorm-288             [-1, 196, 128]             256
          Mamba2-289             [-1, 196, 128]               0
        DropPath-290             [-1, 196, 128]               0
          Conv2d-291          [-1, 128, 14, 14]           1,280
       LayerNorm-292             [-1, 196, 128]             256
          Linear-293             [-1, 196, 512]          66,048
            GELU-294             [-1, 196, 512]               0
         Dropout-295             [-1, 196, 512]               0
          Linear-296             [-1, 196, 128]          65,664
         Dropout-297             [-1, 196, 128]               0
             Mlp-298             [-1, 196, 128]               0
        DropPath-299             [-1, 196, 128]               0
    VMAMBA2Block-300             [-1, 196, 128]               0
          Conv2d-301          [-1, 128, 14, 14]           1,280
       LayerNorm-302             [-1, 196, 128]             256
          Mamba2-303             [-1, 196, 128]               0
        DropPath-304             [-1, 196, 128]               0
          Conv2d-305          [-1, 128, 14, 14]           1,280
       LayerNorm-306             [-1, 196, 128]             256
          Linear-307             [-1, 196, 512]          66,048
            GELU-308             [-1, 196, 512]               0
         Dropout-309             [-1, 196, 512]               0
          Linear-310             [-1, 196, 128]          65,664
         Dropout-311             [-1, 196, 128]               0
             Mlp-312             [-1, 196, 128]               0
        DropPath-313             [-1, 196, 128]               0
    VMAMBA2Block-314             [-1, 196, 128]               0
        Identity-315          [-1, 128, 14, 14]               0
          Conv2d-316         [-1, 1024, 14, 14]         132,096
        Identity-317         [-1, 1024, 14, 14]               0
            ReLU-318         [-1, 1024, 14, 14]               0
       ConvLayer-319         [-1, 1024, 14, 14]               0
        Identity-320         [-1, 1024, 14, 14]               0
          Conv2d-321           [-1, 1024, 7, 7]          10,240
        Identity-322           [-1, 1024, 7, 7]               0
            ReLU-323           [-1, 1024, 7, 7]               0
       ConvLayer-324           [-1, 1024, 7, 7]               0
        Identity-325           [-1, 1024, 7, 7]               0
          Conv2d-326            [-1, 256, 7, 7]         262,400
     BatchNorm2d-327            [-1, 256, 7, 7]             512
        Identity-328            [-1, 256, 7, 7]               0
       ConvLayer-329            [-1, 256, 7, 7]               0
    PatchMerging-330              [-1, 49, 256]               0
      BasicLayer-331  [[-1, 196, 128], [-1, 49, 256]]               0
       LayerNorm-332             [-1, 196, 128]             256
          Conv2d-333            [-1, 256, 7, 7]           2,560
       LayerNorm-334              [-1, 49, 256]             512
          Linear-335              [-1, 49, 768]         196,608
         Dropout-336           [-1, 16, 49, 49]               0
          Linear-337              [-1, 49, 256]          65,792
StandardAttention-338              [-1, 49, 256]               0
        DropPath-339              [-1, 49, 256]               0
          Conv2d-340            [-1, 256, 7, 7]           2,560
       LayerNorm-341              [-1, 49, 256]             512
          Linear-342             [-1, 49, 1024]         263,168
            GELU-343             [-1, 49, 1024]               0
         Dropout-344             [-1, 49, 1024]               0
          Linear-345              [-1, 49, 256]         262,400
         Dropout-346              [-1, 49, 256]               0
             Mlp-347              [-1, 49, 256]               0
        DropPath-348              [-1, 49, 256]               0
    VMAMBA2Block-349              [-1, 49, 256]               0
          Conv2d-350            [-1, 256, 7, 7]           2,560
       LayerNorm-351              [-1, 49, 256]             512
          Linear-352              [-1, 49, 768]         196,608
         Dropout-353           [-1, 16, 49, 49]               0
          Linear-354              [-1, 49, 256]          65,792
StandardAttention-355              [-1, 49, 256]               0
        DropPath-356              [-1, 49, 256]               0
          Conv2d-357            [-1, 256, 7, 7]           2,560
       LayerNorm-358              [-1, 49, 256]             512
          Linear-359             [-1, 49, 1024]         263,168
            GELU-360             [-1, 49, 1024]               0
         Dropout-361             [-1, 49, 1024]               0
          Linear-362              [-1, 49, 256]         262,400
         Dropout-363              [-1, 49, 256]               0
             Mlp-364              [-1, 49, 256]               0
        DropPath-365              [-1, 49, 256]               0
    VMAMBA2Block-366              [-1, 49, 256]               0
          Conv2d-367            [-1, 256, 7, 7]           2,560
       LayerNorm-368              [-1, 49, 256]             512
          Linear-369              [-1, 49, 768]         196,608
         Dropout-370           [-1, 16, 49, 49]               0
          Linear-371              [-1, 49, 256]          65,792
StandardAttention-372              [-1, 49, 256]               0
        DropPath-373              [-1, 49, 256]               0
          Conv2d-374            [-1, 256, 7, 7]           2,560
       LayerNorm-375              [-1, 49, 256]             512
          Linear-376             [-1, 49, 1024]         263,168
            GELU-377             [-1, 49, 1024]               0
         Dropout-378             [-1, 49, 1024]               0
          Linear-379              [-1, 49, 256]         262,400
         Dropout-380              [-1, 49, 256]               0
             Mlp-381              [-1, 49, 256]               0
        DropPath-382              [-1, 49, 256]               0
    VMAMBA2Block-383              [-1, 49, 256]               0
          Conv2d-384            [-1, 256, 7, 7]           2,560
       LayerNorm-385              [-1, 49, 256]             512
          Linear-386              [-1, 49, 768]         196,608
         Dropout-387           [-1, 16, 49, 49]               0
          Linear-388              [-1, 49, 256]          65,792
StandardAttention-389              [-1, 49, 256]               0
        DropPath-390              [-1, 49, 256]               0
          Conv2d-391            [-1, 256, 7, 7]           2,560
       LayerNorm-392              [-1, 49, 256]             512
          Linear-393             [-1, 49, 1024]         263,168
            GELU-394             [-1, 49, 1024]               0
         Dropout-395             [-1, 49, 1024]               0
          Linear-396              [-1, 49, 256]         262,400
         Dropout-397              [-1, 49, 256]               0
             Mlp-398              [-1, 49, 256]               0
        DropPath-399              [-1, 49, 256]               0
    VMAMBA2Block-400              [-1, 49, 256]               0
      BasicLayer-401  [[-1, 49, 256], [-1, 49, 256]]               0
       LayerNorm-402              [-1, 49, 256]             512
Backbone_VMAMBA2-403  [[-1, 32, 56, 56], [-1, 64, 28, 28], [-1, 128, 14, 14], [-1, 256, 7, 7]]               0
 ConvTranspose2d-404          [-1, 128, 14, 14]         524,416
 ConvTranspose2d-405           [-1, 64, 28, 28]         262,208
 ConvTranspose2d-406           [-1, 32, 56, 56]          65,568
 ConvTranspose2d-407         [-1, 64, 112, 112]          65,600
 ConvTranspose2d-408         [-1, 32, 224, 224]          32,800
         Dropout-409         [-1, 32, 224, 224]               0
          Conv2d-410          [-1, 1, 224, 224]              33
         Dropout-411         [-1, 32, 224, 224]               0
          Conv2d-412          [-1, 1, 224, 224]              33
         Dropout-413         [-1, 32, 224, 224]               0
          Conv2d-414          [-1, 1, 224, 224]              33
         Dropout-415         [-1, 32, 224, 224]               0
          Conv2d-416          [-1, 1, 224, 224]              33
================================================================
Total params: 6,467,556
Trainable params: 6,467,556
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.77
Forward/backward pass size (MB): 12089663998051.72
Params size (MB): 24.67
Estimated Total Size (MB): 12089663998077.16
----------------------------------------------------------------
root        : INFO     Beginning Epoch 00
root        : INFO     Epoch: 0, Batch: 100, Loss: 0.0861
root        : INFO     Epoch: 0, Batch: 200, Loss: 0.1214
root        : INFO     Epoch: 0, Batch: 300, Loss: 0.0592
root        : INFO     Epoch: 0, Batch: 400, Loss: 0.0338
root        : INFO     Epoch: 0, Batch: 500, Loss: 0.0389
root        : INFO     Epoch: 0, Batch: 600, Loss: 0.1080
root        : INFO     Epoch: 0, Batch: 700, Loss: 0.1281
root        : INFO     Epoch: 0, Batch: 800, Loss: 0.0581
root        : INFO     Epoch: 0, Batch: 900, Loss: 0.0579
root        : INFO     Validating...
root        : INFO     79/89 = 0.887640
root        : INFO     Validation Loss for Epoch 0: 0.0703
root        : INFO     Beginning Epoch 01
root        : INFO     Epoch: 1, Batch: 100, Loss: 0.0327
root        : INFO     Epoch: 1, Batch: 200, Loss: 0.1509
root        : INFO     Epoch: 1, Batch: 300, Loss: 0.0491
root        : INFO     Epoch: 1, Batch: 400, Loss: 0.0758
root        : INFO     Epoch: 1, Batch: 500, Loss: 0.0784
root        : INFO     Epoch: 1, Batch: 600, Loss: 0.0419
root        : INFO     Epoch: 1, Batch: 700, Loss: 0.0289
root        : INFO     Epoch: 1, Batch: 800, Loss: 0.0396
root        : INFO     Epoch: 1, Batch: 900, Loss: 0.0348
root        : INFO     Validating...
root        : INFO     84/89 = 0.943820
root        : INFO     Validation Loss for Epoch 1: 0.0636
root        : INFO     Beginning Epoch 02
root        : INFO     Epoch: 2, Batch: 100, Loss: 0.0839
root        : INFO     Epoch: 2, Batch: 200, Loss: 0.0643
root        : INFO     Epoch: 2, Batch: 300, Loss: 0.0826
root        : INFO     Epoch: 2, Batch: 400, Loss: 0.0480
root        : INFO     Epoch: 2, Batch: 500, Loss: 0.0182
root        : INFO     Epoch: 2, Batch: 600, Loss: 0.0338
root        : INFO     Epoch: 2, Batch: 700, Loss: 0.0597
root        : INFO     Epoch: 2, Batch: 800, Loss: 0.0782
root        : INFO     Epoch: 2, Batch: 900, Loss: 0.0630
root        : INFO     Validating...
root        : INFO     86/89 = 0.966292
root        : INFO     Validation Loss for Epoch 2: 0.0658
root        : INFO     Beginning Epoch 03
root        : INFO     Epoch: 3, Batch: 100, Loss: 0.0659
root        : INFO     Epoch: 3, Batch: 200, Loss: 0.0568
root        : INFO     Epoch: 3, Batch: 300, Loss: 0.1093
root        : INFO     Epoch: 3, Batch: 400, Loss: 0.0244
root        : INFO     Epoch: 3, Batch: 500, Loss: 0.0369
root        : INFO     Epoch: 3, Batch: 600, Loss: 0.1246
root        : INFO     Epoch: 3, Batch: 700, Loss: 0.0338
root        : INFO     Epoch: 3, Batch: 800, Loss: 0.0413
root        : INFO     Epoch: 3, Batch: 900, Loss: 0.1051
root        : INFO     Validating...
root        : INFO     83/89 = 0.932584
root        : INFO     Validation Loss for Epoch 3: 0.0622
root        : INFO     Beginning Epoch 04
root        : INFO     Epoch: 4, Batch: 100, Loss: 0.0383
root        : INFO     Epoch: 4, Batch: 200, Loss: 0.0844
root        : INFO     Epoch: 4, Batch: 300, Loss: 0.1595
root        : INFO     Epoch: 4, Batch: 400, Loss: 0.0830
root        : INFO     Epoch: 4, Batch: 500, Loss: 0.0533
root        : INFO     Epoch: 4, Batch: 600, Loss: 0.0747
root        : INFO     Epoch: 4, Batch: 700, Loss: 0.0488
root        : INFO     Epoch: 4, Batch: 800, Loss: 0.1188
root        : INFO     Epoch: 4, Batch: 900, Loss: 0.0597
root        : INFO     Validating...
root        : INFO     82/89 = 0.921348
root        : INFO     Validation Loss for Epoch 4: 0.0670
root        : INFO     Beginning Epoch 05
root        : INFO     Epoch: 5, Batch: 100, Loss: 0.1131
root        : INFO     Epoch: 5, Batch: 200, Loss: 0.0841
root        : INFO     Epoch: 5, Batch: 300, Loss: 0.0633
root        : INFO     Epoch: 5, Batch: 400, Loss: 0.1098
root        : INFO     Epoch: 5, Batch: 500, Loss: 0.0849
root        : INFO     Epoch: 5, Batch: 600, Loss: 0.0530
root        : INFO     Epoch: 5, Batch: 700, Loss: 0.0460
root        : INFO     Epoch: 5, Batch: 800, Loss: 0.0706
root        : INFO     Epoch: 5, Batch: 900, Loss: 0.0472
root        : INFO     Validating...
root        : INFO     85/89 = 0.955056
root        : INFO     Validation Loss for Epoch 5: 0.0606
root        : INFO     Beginning Epoch 06
root        : INFO     Epoch: 6, Batch: 100, Loss: 0.0602
root        : INFO     Epoch: 6, Batch: 200, Loss: 0.0329
root        : INFO     Epoch: 6, Batch: 300, Loss: 0.0431
root        : INFO     Epoch: 6, Batch: 400, Loss: 0.0350
root        : INFO     Epoch: 6, Batch: 500, Loss: 0.0932
root        : INFO     Epoch: 6, Batch: 600, Loss: 0.1089
root        : INFO     Epoch: 6, Batch: 700, Loss: 0.1074
root        : INFO     Epoch: 6, Batch: 800, Loss: 0.0636
root        : INFO     Epoch: 6, Batch: 900, Loss: 0.0565
root        : INFO     Validating...
root        : INFO     84/89 = 0.943820
root        : INFO     Validation Loss for Epoch 6: 0.0525
root        : INFO     Beginning Epoch 07
root        : INFO     Epoch: 7, Batch: 100, Loss: 0.0949
root        : INFO     Epoch: 7, Batch: 200, Loss: 0.0423
root        : INFO     Epoch: 7, Batch: 300, Loss: 0.0749
root        : INFO     Epoch: 7, Batch: 400, Loss: 0.2027
root        : INFO     Epoch: 7, Batch: 500, Loss: 0.0578
root        : INFO     Epoch: 7, Batch: 600, Loss: 0.0579
root        : INFO     Epoch: 7, Batch: 700, Loss: 0.0606
root        : INFO     Epoch: 7, Batch: 800, Loss: 0.0180
root        : INFO     Epoch: 7, Batch: 900, Loss: 0.0709
root        : INFO     Validating...
root        : INFO     81/89 = 0.910112
root        : INFO     Validation Loss for Epoch 7: 0.0695
root        : INFO     Beginning Epoch 08
root        : INFO     Epoch: 8, Batch: 100, Loss: 0.0696
root        : INFO     Epoch: 8, Batch: 200, Loss: 0.0310
root        : INFO     Epoch: 8, Batch: 300, Loss: 0.0482
root        : INFO     Epoch: 8, Batch: 400, Loss: 0.0924
root        : INFO     Epoch: 8, Batch: 500, Loss: 0.0568
root        : INFO     Epoch: 8, Batch: 600, Loss: 0.0398
root        : INFO     Epoch: 8, Batch: 700, Loss: 0.0901
root        : INFO     Epoch: 8, Batch: 800, Loss: 0.0206
root        : INFO     Epoch: 8, Batch: 900, Loss: 0.0224
root        : INFO     Validating...
root        : INFO     85/89 = 0.955056
root        : INFO     Validation Loss for Epoch 8: 0.0627
root        : INFO     Beginning Epoch 09
root        : INFO     Epoch: 9, Batch: 100, Loss: 0.0414
root        : INFO     Epoch: 9, Batch: 200, Loss: 0.0350
root        : INFO     Epoch: 9, Batch: 300, Loss: 0.0319
root        : INFO     Epoch: 9, Batch: 400, Loss: 0.0412
root        : INFO     Epoch: 9, Batch: 500, Loss: 0.0512
root        : INFO     Epoch: 9, Batch: 600, Loss: 0.0447
root        : INFO     Epoch: 9, Batch: 700, Loss: 0.0660
root        : INFO     Epoch: 9, Batch: 800, Loss: 0.0633
root        : INFO     Epoch: 9, Batch: 900, Loss: 0.0810
root        : INFO     Validating...
root        : INFO     82/89 = 0.921348
root        : INFO     Validation Loss for Epoch 9: 0.0620
root        : INFO     Total training time: 13.68 minutes
