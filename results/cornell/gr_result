(grconv) xinda-wang@xinda-wang-Legion-Y9000P-IRX8:~/Desktop/robotic-grasping$ python train_network.py --dataset cornell --dataset-path ../Mamba/archive --description training_cornell --network grconvnet3
root        : INFO     CUDA detected. Running with GPU acceleration.
root        : INFO     Loading Cornell Dataset...
root        : INFO     Dataset size is 885
root        : INFO     Training size: 796
root        : INFO     Validation size: 89
root        : INFO     Done
root        : INFO     Loading Network...
root        : INFO     Done
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 224, 224]          10,400
       BatchNorm2d-2         [-1, 32, 224, 224]              64
            Conv2d-3         [-1, 64, 112, 112]          32,832
       BatchNorm2d-4         [-1, 64, 112, 112]             128
            Conv2d-5          [-1, 128, 56, 56]         131,200
       BatchNorm2d-6          [-1, 128, 56, 56]             256
            Conv2d-7          [-1, 128, 56, 56]         147,584
       BatchNorm2d-8          [-1, 128, 56, 56]             256
            Conv2d-9          [-1, 128, 56, 56]         147,584
      BatchNorm2d-10          [-1, 128, 56, 56]             256
    ResidualBlock-11          [-1, 128, 56, 56]               0
           Conv2d-12          [-1, 128, 56, 56]         147,584
      BatchNorm2d-13          [-1, 128, 56, 56]             256
           Conv2d-14          [-1, 128, 56, 56]         147,584
      BatchNorm2d-15          [-1, 128, 56, 56]             256
    ResidualBlock-16          [-1, 128, 56, 56]               0
           Conv2d-17          [-1, 128, 56, 56]         147,584
      BatchNorm2d-18          [-1, 128, 56, 56]             256
           Conv2d-19          [-1, 128, 56, 56]         147,584
      BatchNorm2d-20          [-1, 128, 56, 56]             256
    ResidualBlock-21          [-1, 128, 56, 56]               0
           Conv2d-22          [-1, 128, 56, 56]         147,584
      BatchNorm2d-23          [-1, 128, 56, 56]             256
           Conv2d-24          [-1, 128, 56, 56]         147,584
      BatchNorm2d-25          [-1, 128, 56, 56]             256
    ResidualBlock-26          [-1, 128, 56, 56]               0
           Conv2d-27          [-1, 128, 56, 56]         147,584
      BatchNorm2d-28          [-1, 128, 56, 56]             256
           Conv2d-29          [-1, 128, 56, 56]         147,584
      BatchNorm2d-30          [-1, 128, 56, 56]             256
    ResidualBlock-31          [-1, 128, 56, 56]               0
  ConvTranspose2d-32         [-1, 64, 113, 113]         131,136
      BatchNorm2d-33         [-1, 64, 113, 113]             128
  ConvTranspose2d-34         [-1, 32, 225, 225]          32,800
      BatchNorm2d-35         [-1, 32, 225, 225]              64
  ConvTranspose2d-36         [-1, 32, 225, 225]          82,976
          Dropout-37         [-1, 32, 225, 225]               0
           Conv2d-38          [-1, 1, 224, 224]             129
          Dropout-39         [-1, 32, 225, 225]               0
           Conv2d-40          [-1, 1, 224, 224]             129
          Dropout-41         [-1, 32, 225, 225]               0
           Conv2d-42          [-1, 1, 224, 224]             129
          Dropout-43         [-1, 32, 225, 225]               0
           Conv2d-44          [-1, 1, 224, 224]             129
================================================================
Total params: 1,900,900
Trainable params: 1,900,900
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.77
Forward/backward pass size (MB): 219.96
Params size (MB): 7.25
Estimated Total Size (MB): 227.97
----------------------------------------------------------------
root        : INFO     Beginning Epoch 00
root        : INFO     Epoch: 0, Batch: 100, Loss: 0.0883
root        : INFO     Epoch: 0, Batch: 200, Loss: 0.0992
root        : INFO     Epoch: 0, Batch: 300, Loss: 0.1066
root        : INFO     Epoch: 0, Batch: 400, Loss: 0.1267
root        : INFO     Epoch: 0, Batch: 500, Loss: 0.0518
root        : INFO     Epoch: 0, Batch: 600, Loss: 0.0803
root        : INFO     Epoch: 0, Batch: 700, Loss: 0.0853
root        : INFO     Epoch: 0, Batch: 800, Loss: 0.0557
root        : INFO     Epoch: 0, Batch: 900, Loss: 0.0661
root        : INFO     Validating...
root        : INFO     47/89 = 0.528090
root        : INFO     Validation Loss for Epoch 0: 0.0817
root        : INFO     Beginning Epoch 01
root        : INFO     Epoch: 1, Batch: 100, Loss: 0.0767
root        : INFO     Epoch: 1, Batch: 200, Loss: 0.0865
root        : INFO     Epoch: 1, Batch: 300, Loss: 0.0731
root        : INFO     Epoch: 1, Batch: 400, Loss: 0.0440
root        : INFO     Epoch: 1, Batch: 500, Loss: 0.0575
root        : INFO     Epoch: 1, Batch: 600, Loss: 0.0322
root        : INFO     Epoch: 1, Batch: 700, Loss: 0.1562
root        : INFO     Epoch: 1, Batch: 800, Loss: 0.0683
root        : INFO     Epoch: 1, Batch: 900, Loss: 0.0301
root        : INFO     Validating...
root        : INFO     66/89 = 0.741573
root        : INFO     Validation Loss for Epoch 1: 0.0800
root        : INFO     Beginning Epoch 02
root        : INFO     Epoch: 2, Batch: 100, Loss: 0.1099
root        : INFO     Epoch: 2, Batch: 200, Loss: 0.2051
root        : INFO     Epoch: 2, Batch: 300, Loss: 0.1064
root        : INFO     Epoch: 2, Batch: 400, Loss: 0.1829
root        : INFO     Epoch: 2, Batch: 500, Loss: 0.0804
root        : INFO     Epoch: 2, Batch: 600, Loss: 0.0503
root        : INFO     Epoch: 2, Batch: 700, Loss: 0.0897
root        : INFO     Epoch: 2, Batch: 800, Loss: 0.1094
root        : INFO     Epoch: 2, Batch: 900, Loss: 0.0412
root        : INFO     Validating...
root        : INFO     65/89 = 0.730337
root        : INFO     Validation Loss for Epoch 2: 0.0815
root        : INFO     Beginning Epoch 03
root        : INFO     Epoch: 3, Batch: 100, Loss: 0.0325
root        : INFO     Epoch: 3, Batch: 200, Loss: 0.0538
root        : INFO     Epoch: 3, Batch: 300, Loss: 0.0610
root        : INFO     Epoch: 3, Batch: 400, Loss: 0.0721
root        : INFO     Epoch: 3, Batch: 500, Loss: 0.0469
root        : INFO     Epoch: 3, Batch: 600, Loss: 0.0725
root        : INFO     Epoch: 3, Batch: 700, Loss: 0.0487
root        : INFO     Epoch: 3, Batch: 800, Loss: 0.0836
root        : INFO     Epoch: 3, Batch: 900, Loss: 0.1045
root        : INFO     Validating...
root        : INFO     69/89 = 0.775281
root        : INFO     Validation Loss for Epoch 3: 0.0806
root        : INFO     Beginning Epoch 04
root        : INFO     Epoch: 4, Batch: 100, Loss: 0.0852
root        : INFO     Epoch: 4, Batch: 200, Loss: 0.0937
root        : INFO     Epoch: 4, Batch: 300, Loss: 0.0329
root        : INFO     Epoch: 4, Batch: 400, Loss: 0.0283
root        : INFO     Epoch: 4, Batch: 500, Loss: 0.0837
root        : INFO     Epoch: 4, Batch: 600, Loss: 0.0401
root        : INFO     Epoch: 4, Batch: 700, Loss: 0.0381
root        : INFO     Epoch: 4, Batch: 800, Loss: 0.0453
root        : INFO     Epoch: 4, Batch: 900, Loss: 0.0720
root        : INFO     Validating...
root        : INFO     80/89 = 0.898876
root        : INFO     Validation Loss for Epoch 4: 0.0695
root        : INFO     Beginning Epoch 05
root        : INFO     Epoch: 5, Batch: 100, Loss: 0.0352
root        : INFO     Epoch: 5, Batch: 200, Loss: 0.1304
root        : INFO     Epoch: 5, Batch: 300, Loss: 0.1109
root        : INFO     Epoch: 5, Batch: 400, Loss: 0.0413
root        : INFO     Epoch: 5, Batch: 500, Loss: 0.1067
root        : INFO     Epoch: 5, Batch: 600, Loss: 0.1367
root        : INFO     Epoch: 5, Batch: 700, Loss: 0.1239
root        : INFO     Epoch: 5, Batch: 800, Loss: 0.1291
root        : INFO     Epoch: 5, Batch: 900, Loss: 0.0520
root        : INFO     Validating...
root        : INFO     79/89 = 0.887640
root        : INFO     Validation Loss for Epoch 5: 0.0715
root        : INFO     Beginning Epoch 06
root        : INFO     Epoch: 6, Batch: 100, Loss: 0.0624
root        : INFO     Epoch: 6, Batch: 200, Loss: 0.0713
root        : INFO     Epoch: 6, Batch: 300, Loss: 0.0765
root        : INFO     Epoch: 6, Batch: 400, Loss: 0.1017
root        : INFO     Epoch: 6, Batch: 500, Loss: 0.0785
root        : INFO     Epoch: 6, Batch: 600, Loss: 0.0611
root        : INFO     Epoch: 6, Batch: 700, Loss: 0.0343
root        : INFO     Epoch: 6, Batch: 800, Loss: 0.0405
root        : INFO     Epoch: 6, Batch: 900, Loss: 0.0801
root        : INFO     Validating...
root        : INFO     68/89 = 0.764045
root        : INFO     Validation Loss for Epoch 6: 0.0755
root        : INFO     Beginning Epoch 07
root        : INFO     Epoch: 7, Batch: 100, Loss: 0.0449
root        : INFO     Epoch: 7, Batch: 200, Loss: 0.0755
root        : INFO     Epoch: 7, Batch: 300, Loss: 0.0847
root        : INFO     Epoch: 7, Batch: 400, Loss: 0.0571
root        : INFO     Epoch: 7, Batch: 500, Loss: 0.0722
root        : INFO     Epoch: 7, Batch: 600, Loss: 0.0299
root        : INFO     Epoch: 7, Batch: 700, Loss: 0.1224
root        : INFO     Epoch: 7, Batch: 800, Loss: 0.0725
root        : INFO     Epoch: 7, Batch: 900, Loss: 0.0807
root        : INFO     Validating...
root        : INFO     77/89 = 0.865169
root        : INFO     Validation Loss for Epoch 7: 0.0629
root        : INFO     Beginning Epoch 08
root        : INFO     Epoch: 8, Batch: 100, Loss: 0.0345
root        : INFO     Epoch: 8, Batch: 200, Loss: 0.0796
root        : INFO     Epoch: 8, Batch: 300, Loss: 0.0579
root        : INFO     Epoch: 8, Batch: 400, Loss: 0.0848
root        : INFO     Epoch: 8, Batch: 500, Loss: 0.0437
root        : INFO     Epoch: 8, Batch: 600, Loss: 0.0534
root        : INFO     Epoch: 8, Batch: 700, Loss: 0.0679
root        : INFO     Epoch: 8, Batch: 800, Loss: 0.0475
root        : INFO     Epoch: 8, Batch: 900, Loss: 0.0422
root        : INFO     Validating...
root        : INFO     77/89 = 0.865169
root        : INFO     Validation Loss for Epoch 8: 0.0689
root        : INFO     Beginning Epoch 09
root        : INFO     Epoch: 9, Batch: 100, Loss: 0.1294
root        : INFO     Epoch: 9, Batch: 200, Loss: 0.0734
root        : INFO     Epoch: 9, Batch: 300, Loss: 0.0597
root        : INFO     Epoch: 9, Batch: 400, Loss: 0.0424
root        : INFO     Epoch: 9, Batch: 500, Loss: 0.0508
root        : INFO     Epoch: 9, Batch: 600, Loss: 0.0641
root        : INFO     Epoch: 9, Batch: 700, Loss: 0.0246
root        : INFO     Epoch: 9, Batch: 800, Loss: 0.0463
root        : INFO     Epoch: 9, Batch: 900, Loss: 0.0267
root        : INFO     Validating...
root        : INFO     84/89 = 0.943820
root        : INFO     Validation Loss for Epoch 9: 0.0702
root        : INFO     Total training time: 22.67 minutes
